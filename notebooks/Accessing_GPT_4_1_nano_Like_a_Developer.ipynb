{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecnJouXnUgKv",
        "outputId": "fedc7db7-8235-4d7a-a541-50500a21f782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "320c9810-296f-4837-c561-217061429a15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BaWc1DuZXA2Pw5SQAlQRHUBfErubp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks and tools in the AI and natural language processing ecosystem, but they serve different purposes and have distinct functionalities. Here's a breakdown of their main differences:\\n\\n**1. Purpose and Core Use Cases:**\\n\\n- **LangChain:**\\n  - **Primary Focus:** Building complex, multi-step applications that involve large language models (LLMs), such as chaining prompts, handling conversations, agents, and workflows.\\n  - **Use Cases:** Chatbots, conversational agents, application orchestration, dynamic prompt management, tool integration, and automating interactions with LLMs.\\n  - **Features:** Supports prompt chaining, memory management, agent frameworks, and integrations with various LLM providers and external tools.\\n\\n- **LlamaIndex (GPT Index):**\\n  - **Primary Focus:** Facilitating the ingestion, structuring, and querying of external data sources (documents, knowledge bases) using LLMs.\\n  - **Use Cases:** Building document retrieval systems, question-answering over corpora, indexing large collections of data, and enabling semantic search.\\n  - **Features:** Data ingestion and indexing, flexible connectors to data sources, optimized retrieval methods, and indexing schemas for efficient querying.\\n\\n**2. Functionality and Approach:**\\n\\n- **LangChain:**\\n  - Acts as an SDK to compose and manage complex workflows with LLMs.\\n  - Supports chaining multiple prompts and logic, integrating external APIs and tools.\\n  - Focuses on application-level orchestration — managing how prompts are generated, processed, and how models interact over multiple steps.\\n\\n- **LlamaIndex:**\\n  - Provides tools to preprocess, index, and retrieve information from large external datasets.\\n  - Emphasizes enabling LLMs to access structured or unstructured data efficiently.\\n  - Focuses on data management and retrieval, making it easier for LLMs to answer questions based on specific datasets.\\n\\n**3. Ecosystem and Integration:**\\n\\n- **LangChain:**\\n  - Offers integrations with many LLM providers (OpenAI, Cohere, Azure, etc.).\\n  - Supports deployment of conversational agents with memory.\\n  - Enables complex prompt engineering and dynamic workflows.\\n\\n- **LlamaIndex:**\\n  - Connects with data storage solutions like local files, cloud storage, or document databases.\\n  - Provides tools for document parsing, embedding, indexing, and querying.\\n\\n**4. Summary Table:**\\n\\n| Aspect                     | LangChain                                               | LlamaIndex                                           |\\n|----------------------------|---------------------------------------------------------|-----------------------------------------------------|\\n| Main Purpose               | Building LLM-powered applications and workflows        | Indexing and querying external data sources       |\\n| Focus                      | Workflow orchestration, prompt management               | Data ingestion, indexing, retrieval               |\\n| Typical Use Cases          | Chatbots, conversational agents, multi-step workflows | Document Q&A, semantic search, knowledge bases  |\\n| Key Features               | Prompt chaining, agents, tools integration               | Data ingestion, indexing, retrieval optimization|\\n\\n---\\n\\n**In summary:**\\n- Use **LangChain** if you want to create complex applications that leverage LLMs and require orchestrating multiple steps, prompts, and integrations.\\n- Use **LlamaIndex** if your goal is to process large external datasets, index them, and enable efficient retrieval and question-answering over that data.\\n\\nThey can be complementary: for example, LlamaIndex can be used to prepare and retrieve data, which can then be fed into workflows built with LangChain.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748044869, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_17d1f82c3e', usage=CompletionUsage(completion_tokens=712, prompt_tokens=19, total_tokens=731, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "16062e04-9b2c-4617-f2da-e49f4e1ce661"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications that leverage large language models (LLMs) and external data sources. However, they have distinct focuses and functionalities. Here's a comparison to clarify their differences:\n\n**1. Purpose and Core Focus:**\n\n- **LangChain:**\n  - **Primary Focus:** Modular framework for building 'chain'-based applications with LLMs.\n  - **Use Cases:** Conversation agents, chatbots, decision-making pipelines, and multi-step LLM workflows.\n  - **Features:** Emphasizes composability, with tools for chaining prompts, managing memory, handling user interactions, and integrating various LLM APIs.\n  - **Data Handling:** Supports incorporating external data sources but typically requires custom integration.\n\n- **LlamaIndex (GPT Index):**\n  - **Primary Focus:** Data framework for building indices over external data sources to enable context-aware querying with LLMs.\n  - **Use Cases:** Building knowledge bases, document retrieval, question-answering over large corpora, knowledge management.\n  - **Features:** Provides data ingestion pipelines, index structures (e.g., trees, graphs), and query interfaces designed to efficiently retrieve relevant data snippets for LLM prompts.\n  - **Data Handling:** Specifically optimized for indexing and querying large datasets, enabling LLMs to use external knowledge effectively.\n\n**2. Architecture and Design:**\n\n- **LangChain:** \n  - Emphasizes a composable \"chain\" architecture—connecting prompts, models, memory, and tools.\n  - Supports a wide variety of chains (e.g., simple prompt-response, multi-turn dialogs, tools integration).\n\n- **LlamaIndex:**\n  - Focuses on creating and managing data indices that allow LLMs to reference external documents.\n  - Incorporates components like document loaders, index builders, and query engines.\n\n**3. Integration and Extensibility:**\n\n- **LangChain:**\n  - Integrates with multiple LLM providers (OpenAI, Hugging Face, etc.).\n  - Offers integrations with APIs, databases, and other tools.\n  - Extensively supports custom chains and tools for advanced workflows.\n\n- **LlamaIndex:**\n  - Designed to easily ingest large datasets and create retrieval-optimized indices.\n  - Supports various data sources (PDFs, webpages, databases).\n\n**4. Typical Usage Scenarios:**\n\n- **LangChain:** Building chatbots, conversational agents, AI assistants that require multi-step reasoning, memory, or external tool integration.\n\n- **LlamaIndex:** Creating a searchable knowledge base from large collections of documents, enabling LLMs to answer questions with access to external data.\n\n---\n\n**Summary:**\n\n| Aspect                  | LangChain                                              | LlamaIndex                                              |\n|-------------------------|--------------------------------------------------------|--------------------------------------------------------|\n| Core Purpose            | Building multi-step LLM applications, chains, and workflows | Building and querying indices over external datasets  |\n| Focus                   | Modular orchestration, chaining, and tool integration | Data ingestion, indexing, and retrieval for QA       |\n| Typical Use Cases       | Chatbots, agents, decision workflows                  | Document Q&A, knowledge bases                        |\n| Data Handling           | Supports external data but less focused on indexing     | Specialized in indexing, retrieval, and data management |\n\n**In essence:**  \n- Use **LangChain** if you're aiming to build complex, multi-step applications involving LLMs, memory, tools, and conversational flows.  \n- Use **LlamaIndex** if your primary goal is to process large external datasets, create indices, and enable question-answering over that data with LLMs.\n\n---\n\nIf you're planning a project, consider whether your focus is on orchestrating multi-step interactions (**LangChain**) or on efficiently managing and querying large datasets (**LlamaIndex**). Often, they can be used together for comprehensive applications."
          },
          "metadata": {}
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "69095ba7-f406-4437-f401-86f215819286"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Are you kidding me? I haven't eaten in ages, and now you ask about ice preferences? Honestly, I don't care if it's crushed or cubed—what I want is real food! But if I had to choose, crushed ice is just frustratingly messy, and cubed ice takes forever to chew through when I'm starving. Ugh! Just give me some actual food already!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "976f3dc3-4cc7-42d3-a09c-82aaa57614e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I love both! Crushed ice is perfect for refreshing drinks and gives a nice cool burst, while cubed ice adds a touch of elegance and lasts longer in your beverage. It really depends on what you're craving—what about you? Which do you prefer?"
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "54134543-2834-400b-d44d-e946b91f6adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-BaWd1pMBkElNFiiI7AWuHxxCEXTAT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I love both! Crushed ice is perfect for refreshing drinks and gives a nice cool burst, while cubed ice adds a touch of elegance and lasts longer in your beverage. It really depends on what you're craving—what about you? Which do you prefer?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748044931, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_eede8f0d45', usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "aead34a8-e0b3-4d6b-8d83-e9818e470c62"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure! Here's a sentence using both words:\n\n\"During the strange festival, I noticed a stimple dance performed by the villagers while a falbean melody played softly in the background.\""
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ee3f4735-28a8-4b03-af28-21f694746810"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The stimple wrench quickly secured the falbean in place, ensuring the machinery operated smoothly."
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "11f6ed51-2b88-460d-fd9b-08aeefd65ee1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's analyze the options:\n\n**Option 1:** Fly (3 hours) + Bus (2 hours)  \nTotal travel time: 3 + 2 = 5 hours\n\n**Option 2:** Teleporter (0 hours) + Bus (1 hour)  \nTotal travel time: 0 + 1 = 1 hour\n\n---\n\n**Additional considerations:**\n- **Current time:** 1PM local time\n- **Deadline:** Before 7PM EDT\n\nAssuming the current local time is 1PM and Billy wants to arrive **before 7PM EDT**, we need to consider the time difference between his local time zone and EDT.\n\n**Important:**  \n- San Francisco is in PDT (Pacific Daylight Time), which is UTC-7.  \n- EDT (Eastern Daylight Time) is UTC-4.  \n- Therefore, EDT is 3 hours ahead of PDT.\n\n---\n\n**Converting current local time to EDT:**  \n- If it's 1PM local time in San Francisco (PDT), the current EDT is:  \n1PM + 3 hours = 4PM EDT\n\n**Deadline in local time:**  \nBilly needs to arrive **before 7PM EDT**, which is:  \n7PM EDT - 3 hours = 4PM PDT\n\nSo, Billy must arrive **before 4PM PDT**.\n\n---\n\n**Calculating latest departure times:**\n\n- To arrive before 4PM PDT:  \n  - **Option 1 (Fly + Bus):**  \n    Total time: 5 hours  \n    Latest departure time: 4PM - 5 hours = **11AM PDT**  \n  - **Option 2 (Teleporter + Bus):**  \n    Total time: 1 hour  \n    Latest departure time: 4PM - 1 hour = **3PM PDT**\n\n---\n\n**Current time and possibilities:**\n\n- Now it is 1PM PDT.\n- **Option 1:**  \n  You must leave **by 11AM PDT** to arrive on time — **already too late** now.\n- **Option 2:**  \n  Must leave **by 3PM PDT** — it is currently 1PM, so there is **2 hours remaining** to catch the teleporter.\n\n---\n\n**Conclusion:**  \n- **If Billy wants to arrive before 7PM EDT (which is 4PM PDT),** he **must** take the teleporter option, because the fly+bus option is no longer feasible (he would need to depart by 11AM PDT, which has already passed).\n\n- **Given current time (1PM PDT),** Billy can take the teleporter + bus to arrive on time, but the fly + bus option is too late.\n\n---\n\n### Final answer:\n\n**It does matter which option Billy chooses.**  \n- The teleporter + bus allows him to arrive on time (before 4PM PDT, which is before 7PM EDT).  \n- The fly + bus option cannot make it in time given the current time.\n\n**Therefore, Billy should choose the teleporter + bus option to ensure he gets home before 7PM EDT.**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}